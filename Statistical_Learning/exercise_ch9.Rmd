In this problem, you will use simulation to evaluate (by Monte Carlo) the expected misclassification error rate given a particular generating model.  Let yi be equally divided between classes 0 and 1, and let xi ∈ ℝ10 be normally distributed. Given yi=0, xi ∼ N10(0, I10).  Given yi=1, xi ∼N10(μ, I10) with μ=(1,1,1,1,1,0,0,0,0,0).

Now, we would like to know the expected test error rate if we fit an SVM to a sample of 50 random training points from class 1 and 50 more from class 0.  We can calculate this to high precision by 1) generating a random training sample to train on, 2) evaluating the number of mistakes we make on a large test set, and then 3) repeating (1-2) many times and averaging the error rate for each trial.
=================================================================================
```{r}
library(MASS) #Use this package for the mvrnorm() function
library(e1071) #Use this package for the SVM() function
```

Create dataset:
```{r}
set.seed(10111) #Create a dataset of 100 observations (with 50 1's and 50 0's) with 10 predictors
x1 = mvrnorm(n = 50, mu = rep(0,10), Sigma=diag(10)) # n is number of obs., mu is vector mean, sigma is Identity matrix (variance-covariance)
dim(x1)
x2 = mvrnorm(n = 50, mu = rep(0,10), Sigma=diag(10))
xvar_1 = rbind(x1,x2)
dim(xvar_1)
data1 = matrix(xvar_1)
y = rep(c(0,1), c(50,50)) #Make a vector of 100 obs. with 50 1's and 50 0's
y = matrix(y)
dim (y)
```

Form training dataset:
```{r}
train_data = data.frame(xvar_1, y = as.factor(y)) #Put the y variable in with the X variables to form a training dataset
dim(train_data)
```

Create test dataset:
```{r}
x3 = mvrnorm(500,rep(0,10),diag(10))#Create a large dataset with 1000 observations for the test datset with 10 predictors
x4 = mvrnorm(500,rep(c(1,0),c(5,5)),diag(10))
xvar_2 = rbind(x3,x4)
y1 = rep(c(0,1),c(500,500))
test_data = data.frame(xvar_2,y1=as.factor(y1))
dim (test_data) #Test data set with 1000 observations, Y variable with 500 1's and 500 0's, and 10 X variables
```

Q9.1:
=====
Use svm in the e1071 package with the default settings (the default kernel is a radial kernel). What is the expected test error rate of this method (to within 10%)?

```{r}
sample1_train <- train_data[sample(1:nrow(train_data), 50, replace=TRUE),]
svmModel1 = svm(sample1_train$y ~., data=sample1_train)
svmModel1
svmPredicted1 <- predict(svmModel1, newdata = test_data)
svmPredicted1
errorrateSVM1 <- (sum(svmPredicted1 != test_data$y))/length(test_data$y)
errorrateSVM1 #0.512
```

```{r}
sample2_train <- train_data[sample(1:nrow(train_data), 50,replace=TRUE),]
svmModel2 = svm(sample2_train$y ~., data=sample2_train)
svmModel2
svmPredicted2 <- predict(svmModel2, newdata = test_data)
svmPredicted2
errorrateSVM2 <- (sum(svmPredicted2 != test_data$y))/length(test_data$y)
errorrateSVM2 #0.524
```

```{r}
sample3_train <- train_data[sample(1:nrow(train_data), 50,replace=TRUE),]
svmModel3 = svm(sample2_train$y ~., data=sample3_train)
svmModel3
svmPredicted3 <- predict(svmModel3, newdata = test_data)
svmPredicted3
errorrateSVM3 <- (sum(svmPredicted3 != test_data$y))/length(test_data$y)
errorrateSVM3 #0.4
```

```{r}
sum(errorrateSVM1, errorrateSVM2, errorrateSVM3)/3 #Average of the test errors for SVM = 0.4786667
```

Use Monte Carlo method to repeat above for 1000 iterations on SVM with default setting radial kernel
```{r}
error_vector <- c();
for (i in 1:1000) {
  sample_train <- train_data[sample(1:nrow(train_data), 50, replace = TRUE),] # Generate traning data
  svmModel = svm(sample_train$y ~., data=sample_train) #Fit a model on trainning data
  ## Generate test data. Use test_data from above
  svmPredicted <- predict(svmModel, newdata = test_data) #Predict on test data
  errorrateSVM <- (sum(svmPredicted != test_data$y))/length(test_data$y) #Compare prediction with y
  error_vector <- c(error_vector, errorrateSVM) #Save the result in a vector
}

head(error_vector) #Random check the values in the vector: 0.498 0.509 0.280 0.538 0.397 0.549
error_vector[999] #0.566
error_vector[750] #0.56
mean(error_vector) # Take the average across the 1000 test errors: 0.50025
```

Q.9.2
======
Now fit an svm with a linear kernel (kernel = "linear"). What is the expected test error rate to within 10%?

```{r}
sample6_train <- train_data[sample(1:nrow(train_data), 50,replace=TRUE),]
svmModel6 = svm(sample6_train$y ~., data=sample6_train, kernel = "linear")
svmModel6
svmPredicted6 <- predict(svmModel6, newdata = test_data)
svmPredicted6
errorrateSVM6 <- (sum(svmPredicted6 != test_data$y))/length(test_data$y)
errorrateSVM6 #0.517
```

```{r}
sample7_train <- train_data[sample(1:nrow(train_data), 50,replace=TRUE),]
svmModel7 = svm(sample7_train$y ~., data=sample7_train, kernel = "linear")
svmModel7
svmPredicted7 <- predict(svmModel7, newdata = test_data)
svmPredicted7
errorrateSVM7 <- (sum(svmPredicted7 != test_data$y))/length(test_data$y)
errorrateSVM7 #0.727
```

```{r}
sum(errorrateSVM6, errorrateSVM7)/2 #Take the average of the test errors for SVM models with linear kernel = 0.622
```

USe Monte Carlo method to repeat above for 1000 iterations on SVM with linear kernel
```{r}
error_vector1 <- c();
for (i in 1:1000) {
  sample_train <- train_data[sample(1:nrow(train_data), 50, replace = TRUE),] #Generate traning data
  svmModel = svm(sample_train$y ~., data=sample_train, kernel = "linear") #Fit a model on trainning data
  ## Generate test data. Use test_data from above
  svmPredicted <- predict(svmModel, newdata = test_data) #Predict on test data
  errorrateSVM <- (sum(svmPredicted != test_data$y))/length(test_data$y) #Compare prediction with y
  error_vector1 <- c(error_vector1, errorrateSVM) #Save the result in a vector
}

head(error_vector1) # Random check the values in the vector: 0.482 0.530 0.341 0.600 0.593 0.311
error_vector1[999] #0.598
error_vector1[750] #0.569
mean(error_vector1) #Take the average across the 1000 test errors = 0.49962
```

Q. 9.3
======
What is the expected test error for logistic regression? (to within 10%)

```{r}
sample11_train <- train_data[sample(1:nrow(train_data), 50,replace=TRUE),]
logistic1 = glm(sample11_train$y ~., family = "binomial", data=sample11_train)
summary(logistic1)                          
fitted <- predict(logistic1, newdata=test_data, type = 'response')
missClass <- function(values, prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
errorratelog1 <-missClass(test_data$y,fitted)
errorratelog1 #0.619
```

```{r}
sample12_train <- train_data[sample(1:nrow(train_data), 50, replace=TRUE),]
logistic2 = glm(sample12_train$y ~., family = "binomial", data=sample12_train)
summary(logistic2)                          
fitted1 <- predict(logistic2, newdata=test_data, type = 'response')
missClass <- function(values, prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
errorratelog2 <- missClass(test_data$y,fitted1)
errorratelog2 #0.514
```

```{r}
sample13_train <- train_data[sample(1:nrow(train_data), 50, replace=TRUE),]
logistic3 = glm(sample13_train$y ~., family = "binomial", data=sample13_train)
summary(logistic3)                          
fitted2 <- predict(logistic3, newdata=test_data, type = 'response')
missClass <- function(values, prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
errorratelog3 <- missClass(test_data$y,fitted2)
errorratelog3 #0.38
```

```{r}
sum(errorratelog1, errorratelog2, errorratelog3)/3 #Take the average of the test errors for binary logistic = 0.5043333
```

USe Monte Carlo method to repeat above for 1000 iterations on binary logistic regression model

```{r}
missClass <- function(values, prediction){sum(((prediction > 0.5)*1) != values)/length(values)} #Use a function for missclassification error rate
error_vector2 <- c(); #Set up a vector to store the error rates 
for (i in 1:1000) {
  sample_train <- train_data[sample(1:nrow(train_data), 50, replace = TRUE),] #Generate traning data
  LogisticModel = glm(sample_train$y ~., family = "binomial", data=sample_train) #Fit a model on trainning data
  ## Generate test data. Use test_data from above
  fitted <- predict(LogisticModel, newdata=test_data, type = 'response') #Predict on test data
  errorrate_logistic <- missClass(test_data$y,fitted) #Compare prediction with y
  error_vector2 <- c(error_vector2, errorrate_logistic) #Save the result in a vector
}
head(error_vector2) # Random check the values in the vector: 0.580 0.662 0.460 0.560 0.605 0.519
error_vector2[999] #0.305
error_vector2[750] #0.444
mean(error_vector2) #Take the average across the 1000 test errors = 0.501163
```

(Don't worry if you get errors saying the logistic regression did not converge.)

Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
3: glm.fit: fitted probabilities numerically 0 or 1 occurred 
4: glm.fit: fitted probabilities numerically 0 or 1 occurred
